{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitnlpbookcondad4249c7ccc4040c9bfb0ad7a6e926f59",
   "display_name": "Python 3.8.1 64-bit ('nlpbook': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq Model Simple Illustration\n",
    "\n",
    "Start from a very simple example:\n",
    "  \n",
    "I am Licor -> Je suis Licor  \n",
    "I am fine -> Je vais bien\n",
    "\n",
    "We want the model to learn the translation of those two pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ['I am Licor', 'I am fine']\n",
    "target = ['Je suis Licor', 'Je vais bien']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary\n",
    "src_itos = {0: 'I', 1: 'am', 2: 'Licor', 3: 'fine', 4:'<GO>'}\n",
    "tgt_itos = {0: 'Je', 1: 'suis', 2: 'Licor', 3: 'vais', 4: 'bien', 5:'<GO>'}\n",
    "\n",
    "src_stoi = {v: k for k, v in src_itos.items()}\n",
    "tgt_stoi = {v: k for k, v in tgt_itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the sentences and group into pair\n",
    "pairs = []\n",
    "for p in zip(source, target):\n",
    "    s_idx = [src_stoi.get(w) for w in p[0].split()]\n",
    "    t_idx = [tgt_stoi.get(w) for w in p[1].split()]\n",
    "    pairs.append((s_idx, t_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[([0, 1, 2], [0, 1, 2]), ([0, 1, 3], [0, 3, 4])]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq model with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5.518531322479248\n5.243597984313965\n3.9942703247070312\n4.468466758728027\n3.1920394897460938\n3.6199026107788086\n2.5253758430480957\n2.723552942276001\n1.9895079135894775\n1.9824979305267334\n1.5752532482147217\n1.4385730028152466\n1.1577732563018799\n1.0526231527328491\n0.7395479083061218\n0.7881637811660767\n0.4203367233276367\n0.5264965295791626\n0.23585759103298187\n0.2521694004535675\n"
    }
   ],
   "source": [
    "# prepare training data\n",
    "input_seq = []\n",
    "output_seq = []\n",
    "for src, tgt in pairs:\n",
    "    src_idx = torch.LongTensor([src], device=device).view(-1, 1)\n",
    "    tgt_idx = torch.LongTensor([tgt], device=device).view(-1, 1)\n",
    "    input_seq.append(src_idx)\n",
    "    output_seq.append(tgt_idx)\n",
    "\n",
    "# create encoder, decoder and optimizer\n",
    "INPUT_SIZE = len(src_itos)\n",
    "OUTPUT_SIZE = len(tgt_itos)\n",
    "HIDDEN_SIZE = 32\n",
    "LR = 0.01\n",
    "\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE).to(device)\n",
    "decoder = Decoder(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# initializa hidden layer in encoder\n",
    "encoder_hidden = encoder.initHidden()\n",
    "\n",
    "# clear gradient\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "# start training\n",
    "for _ in range(10):\n",
    "    for idx in range(len(input_seq)):\n",
    "        input_tensor = input_seq[idx]\n",
    "        output_tensor = output_seq[idx]\n",
    "        loss = 0\n",
    "        for i in range(len(input_tensor)):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[tgt_stoi['<GO>']]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        for i in range(len(output_tensor)):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, output_tensor[i])\n",
    "            decoder_input = output_tensor[i]  \n",
    "        print(loss.item())\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Je vais bien\n"
    }
   ],
   "source": [
    "input_sent = 'I am fine'\n",
    "\n",
    "def sent_to_tensor(s):\n",
    "    s_idx = [src_stoi.get(w) for w in s.split()]\n",
    "    return torch.LongTensor([s_idx], device=device).view(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_tensor = sent_to_tensor(input_sent)\n",
    "    input_length = input_tensor.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    for i in range(len(input_tensor)):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[tgt_stoi['<GO>']]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "\n",
    "    for i in range(3):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        decoded_words.append(tgt_itos[topi.item()])\n",
    "\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "    print(' '.join(decoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}