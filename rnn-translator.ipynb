{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Translator\n",
    "I build an English-Franch Translator in this notebook use RNN sequence to sequence model\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*3lj8AGqfwEE5KCTJ-dXTvg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_text_path = './data/small_vocab_en'\n",
    "fr_text_path = './data/small_vocab_fr'\n",
    "with open(en_text_path, 'r', encoding='utf-8') as f:\n",
    "    en_text = f.read()\n",
    "with open(fr_text_path, 'r', encoding='utf-8') as f:\n",
    "    fr_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new jersey is sometimes quiet during autumn , and it is snowy in april .',\n",
       " 'the united states is usually chilly during july , and it is usually freezing in november .',\n",
       " 'california is usually quiet during march , and it is usually hot in june .',\n",
       " 'the united states is sometimes mild during june , and it is cold in september .',\n",
       " 'your least liked fruit is the grape , but my least liked is the apple .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text.split('\\n')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\",\n",
       " 'les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .',\n",
       " 'california est généralement calme en mars , et il est généralement chaud en juin .',\n",
       " 'les états-unis est parfois légère en juin , et il fait froid en septembre .',\n",
       " 'votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_text.split('\\n')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input data\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*Ismhi-muID5ooWf3ZIQFFg.png)   \n",
    "\n",
    "Though the seq2seq model is designed for variable length input and output sequence, we still have to fill the network with same size input, the way to implement this is use special paddings    \n",
    "* **&lt;PAD&gt;**: During training, we’ll need to feed our examples to the network in batches. The inputs in these batches all need to be the same width for the network to do its calculation. Our examples, however, are not of the same length. That’s why we’ll need to pad shorter inputs to bring them to the same width of the batch   \n",
    "* **&lt;EOS&gt;**: This is another necessity of batching as well, but more on the decoder side. It allows us to tell the decoder where a sentence ends, and it allows the decoder to indicate the same thing in its outputs as well.   \n",
    "* **&lt;UNK&gt;**: If you’re training your model on real data, you’ll find you can vastly improve the resource efficiency of your model by ignoring words that don’t show up often enough in your vocabulary to warrant consideration. We replace those with **&lt;UNK&gt;**.    \n",
    "* **&lt;GO&gt;**: This is the input to the first time step of the decoder to let the decoder know when to start generating output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_vocab_int(text):\n",
    "    text = text.lower()\n",
    "    vocab = sorted(set(text.split()))\n",
    "    vocab_counter = Counter(vocab)\n",
    "    # Later to get rid of the low frequency words! I'll leave it for now\n",
    "    vocab = ['<PAD>','<EOS>','<UNK>','<GO>'] + vocab\n",
    "    vocab_to_int = {word: index for index, word in enumerate(vocab)}\n",
    "    int_to_vocab = {index: word for word, index in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab, vocab_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_vocab_to_int, en_int_to_vocab, en_vocab_counter = get_vocab_int(en_text)\n",
    "fr_vocab_to_int, fr_int_to_vocab, fr_vocab_counter = get_vocab_int(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "\n",
    "    source_id_text_line = [line for line in source_text.split('\\n')]\n",
    "    target_id_text_line = [line for line in target_text.split('\\n')]\n",
    "    source_id_text = []\n",
    "    target_id_text = []\n",
    "    for line in source_id_text_line:\n",
    "        new_line = [source_vocab_to_int.get(word, source_vocab_to_int['<UNK>']) for word in line.split()]\n",
    "        source_id_text.append(new_line)\n",
    "        \n",
    "    for line in target_id_text_line:\n",
    "        new_line = [target_vocab_to_int.get(word, target_vocab_to_int['<UNK>']) for word in line.split()]\n",
    "        new_line.append(target_vocab_to_int['<EOS>'])\n",
    "        target_id_text.append(new_line)\n",
    "        \n",
    "    return source_id_text, target_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_text_to_id, fr_text_to_id = text_to_ids(en_text, fr_text, en_vocab_to_int, fr_vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in this form   \n",
    "<img src='./img/format.png' width='600px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_en_seq_length = max([len(line) for line in en_text_to_id])\n",
    "max_fr_seq_length = max([len(line) for line in fr_text_to_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text_to_id = [sentence + [en_vocab_to_int['<PAD>']] * (max_en_seq_length - len(sentence))\n",
    "                    for sentence in en_text_to_id]\n",
    "fr_text_to_id = [sentence + [fr_vocab_to_int['<PAD>']] * (max_fr_seq_length - len(sentence))\n",
    "                    for sentence in fr_text_to_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('preprocess.p', 'wb') as out_file:\n",
    "    pickle.dump((\n",
    "        (en_text_to_id, fr_text_to_id),\n",
    "        (en_vocab_to_int, fr_vocab_to_int),\n",
    "        (en_int_to_vocab, fr_int_to_vocab)), out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_placeholder():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_length')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "![](./img/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no use for the last one word in the decoder input sequence, so remove it and add < GO > to the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                       impute_finished=True,\n",
    "                                                                       maximum_iterations=max_target_sequence_length)[0]\n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                target_letter_to_int['<EOS>'])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        enc_state,\n",
    "                                                        output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "         \n",
    "\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put encoder and decoder together to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # Pass the input data through the encoder. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_vocab_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 8\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocess.p', mode='rb') as in_file:\n",
    "    ((source_text_int, target_text_int),\n",
    "    (source_vocab_to_int, target_vocab_to_int),\n",
    "    (source_int_to_vocab, target_int_to_vocab)) = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_input_placeholder()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_vocab_to_int),\n",
    "                                                                      len(target_vocab_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(targets, sources, batch_size):\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        targets_lengths = []\n",
    "        for target in targets_batch:\n",
    "            targets_lengths.append(len(target))\n",
    "\n",
    "        source_lengths = []\n",
    "        for source in sources_batch:\n",
    "            source_lengths.append(len(source))\n",
    "        \n",
    "        yield targets_batch, sources_batch, targets_lengths, source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/8 Batch  200/1076 - Loss:  2.475  - Validation loss:  2.490\n",
      "Epoch   1/8 Batch  400/1076 - Loss:  2.130  - Validation loss:  2.082\n",
      "Epoch   1/8 Batch  600/1076 - Loss:  1.817  - Validation loss:  1.790\n",
      "Epoch   1/8 Batch  800/1076 - Loss:  1.593  - Validation loss:  1.519\n",
      "Epoch   1/8 Batch 1000/1076 - Loss:  1.323  - Validation loss:  1.309\n",
      "Epoch   2/8 Batch  200/1076 - Loss:  1.124  - Validation loss:  1.101\n",
      "Epoch   2/8 Batch  400/1076 - Loss:  1.004  - Validation loss:  0.968\n",
      "Epoch   2/8 Batch  600/1076 - Loss:  0.884  - Validation loss:  0.872\n",
      "Epoch   2/8 Batch  800/1076 - Loss:  0.816  - Validation loss:  0.800\n",
      "Epoch   2/8 Batch 1000/1076 - Loss:  0.744  - Validation loss:  0.740\n",
      "Epoch   3/8 Batch  200/1076 - Loss:  0.696  - Validation loss:  0.679\n",
      "Epoch   3/8 Batch  400/1076 - Loss:  0.647  - Validation loss:  0.642\n",
      "Epoch   3/8 Batch  600/1076 - Loss:  0.606  - Validation loss:  0.613\n",
      "Epoch   3/8 Batch  800/1076 - Loss:  0.598  - Validation loss:  0.592\n",
      "Epoch   3/8 Batch 1000/1076 - Loss:  0.557  - Validation loss:  0.574\n",
      "Epoch   4/8 Batch  200/1076 - Loss:  0.553  - Validation loss:  0.544\n",
      "Epoch   4/8 Batch  400/1076 - Loss:  0.535  - Validation loss:  0.524\n",
      "Epoch   4/8 Batch  600/1076 - Loss:  0.500  - Validation loss:  0.509\n",
      "Epoch   4/8 Batch  800/1076 - Loss:  0.484  - Validation loss:  0.486\n",
      "Epoch   4/8 Batch 1000/1076 - Loss:  0.452  - Validation loss:  0.470\n",
      "Epoch   5/8 Batch  200/1076 - Loss:  0.457  - Validation loss:  0.451\n",
      "Epoch   5/8 Batch  400/1076 - Loss:  0.448  - Validation loss:  0.435\n",
      "Epoch   5/8 Batch  600/1076 - Loss:  0.415  - Validation loss:  0.420\n",
      "Epoch   5/8 Batch  800/1076 - Loss:  0.402  - Validation loss:  0.409\n",
      "Epoch   5/8 Batch 1000/1076 - Loss:  0.383  - Validation loss:  0.401\n",
      "Epoch   6/8 Batch  200/1076 - Loss:  0.399  - Validation loss:  0.396\n",
      "Epoch   6/8 Batch  400/1076 - Loss:  0.394  - Validation loss:  0.387\n",
      "Epoch   6/8 Batch  600/1076 - Loss:  0.367  - Validation loss:  0.375\n",
      "Epoch   6/8 Batch  800/1076 - Loss:  0.366  - Validation loss:  0.374\n",
      "Epoch   6/8 Batch 1000/1076 - Loss:  0.343  - Validation loss:  0.360\n",
      "Epoch   7/8 Batch  200/1076 - Loss:  0.358  - Validation loss:  0.352\n",
      "Epoch   7/8 Batch  400/1076 - Loss:  0.354  - Validation loss:  0.347\n",
      "Epoch   7/8 Batch  600/1076 - Loss:  0.326  - Validation loss:  0.338\n",
      "Epoch   7/8 Batch  800/1076 - Loss:  0.327  - Validation loss:  0.331\n",
      "Epoch   7/8 Batch 1000/1076 - Loss:  0.300  - Validation loss:  0.320\n",
      "Epoch   8/8 Batch  200/1076 - Loss:  0.320  - Validation loss:  0.309\n",
      "Epoch   8/8 Batch  400/1076 - Loss:  0.314  - Validation loss:  0.302\n",
      "Epoch   8/8 Batch  600/1076 - Loss:  0.292  - Validation loss:  0.297\n",
      "Epoch   8/8 Batch  800/1076 - Loss:  0.286  - Validation loss:  0.291\n",
      "Epoch   8/8 Batch 1000/1076 - Loss:  0.265  - Validation loss:  0.284\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_text_int[batch_size:]\n",
    "train_target = target_text_int[batch_size:]\n",
    "valid_source = source_text_int[:batch_size]\n",
    "valid_target = target_text_int[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target, valid_source, batch_size))\n",
    "\n",
    "display_step = 200 # Check training loss after every 20 batches\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size)):\n",
    "            assert len(targets_batch) == len(sources_batch)\n",
    "            assert len(targets_lengths) == len(sources_lengths)\n",
    "            # Training step\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            # Debug message updating us on the status of the training\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                # Calculate validation cost\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need a function to convert sentence to sequence for the feeding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(text, vocab_to_int):\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = 'i saw a car'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Input\n",
      "  Word Ids:      [99, 176, 7, 39]\n",
      "  English Words: ['i', 'saw', 'a', 'car']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [118, 177, 41, 10, 1]\n",
      "  French Words: est la automne . <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The result   \n",
    "the result is rather bad..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
